###############################################################################################
# Tags to control which components are deployed
# These are provided to allow ems01, ems02 and each individual data replica to be created
# as their own helm release, allowing much greater control over upgrade and rollback.
##############################################################################################
tags:
  deployEms01: false
  deployCommon: false
  deployEms02: false
  deployRegions: false
  deployPyTest: false
  deployHAProxy: false
  deployNrfClient: false

################################################################################################
#  Global Values for this stratum site
################################################################################################
global:
  site: site1
  geoRegion: geoRegion1
  useConfigProxies: false
  # if the kubernetes cluster's root domain is not svc.cluster.local, change this otherwise DNS lookups will fail. 
  kubernetesClusterDomain: svc.cluster.local
  storageClass: csi-sc-cinderplugin


  timezone: UTC

  # whether to create volumes to store core files generated by containers (assuming enabled on worker nodes)
  createCoreVolumes: true

  # whether the cluster has multple workers. usually only set false if working in minikube
  multiWorkerNode: true

  imagesRegistry: harbor.bfs.openwave.com/stratum-qe-pipeline
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []

  # SSH keys secret used in the solution. ssh_Secrets would be the secret name if keys are provided externally else by default it uses internal default keys below. For extarnal keys both id_rsa and id_dsa keys should be available in the secret and it should be provided as an argument or in override values file.
  # For external secret, it should be created and available beforehand on the system
  ssh_Secrets: 
  ssh_SecretsName: stratum-ssh-keys
  id_rsa_pub_key: c3NoLXJzYSBBQUFBQjNOemFDMXljMkVBQUFBREFRQUJBQUFCQVFDNXM0ZFhzQTFVS01DWlVBUGRUcjg5Q0pJbzBVWkQ3eUFObTFqU2JTZGxqYm1oWE1tNmpNZzZRZXg2U0ZtcmIwNmg1Uk5jREs4dkxwQ2VHOXFkd05QbnZFMWZjSC9UUTFraUx6aEhvZVF6TGU1dW1Yb0RlN1JtK1V1b1kxQ2JMOHdVaGJpVnJtZEE0MGl3OTRDcHRzWDRZQXlGNkJHeERUZHl1VGJIdTRobzBPVVVlQSt1SGJTWEo1aGFOemJqZG5YMStxOGpXaUJUSHRaWWJHQVR2OTZnVmRDVkNvdFBZRm04V2NYeE5WNXBnemVOejBINTZqZ2ZzS0pDUXE1RnQwLzhzRXp4Q2lzVEhEempHK3ZpcFU1MEdCUkdMMVgvVkVsY2RQWDNJVzdCakZIeWtXdTEwNHV1NVVxNkV5TnE1Si9qOUt5bDBxZkxvdEZNakYxZnZibk4gcm9vdEBlbXMwMQo=
  id_dsa_pub_key: c3NoLWRzcyBBQUFBQjNOemFDMWtjM01BQUFDQkFMcjdjTWFxTENHc3F3YWJSbzVYTDZuK2ZpTWhXUE9pVXpVL3QvVDA3MldUTmhsWElXb202citWRW9UQmE3ZlU1SXBTS1FFM0tyMktMeXVLNlR3QUR4VldsMGNsek9Rc245VFAveG9RVWNTOG5RWk1rdVI2RDcvV0RlVDhBWlhYWGdMcTdiaDZaVFlIdWpEVXdERzh6elIxUTV6TjR6d0VqNGtyTXUwU3EyYVBBQUFBRlFDNDRtK2dTcWFMR0kxN0hua0Zwa0IrYmF4bWFRQUFBSUJieHpCbEpHa0laazVGR3pJYUdPd3JlVE1TdzFzS3lyVmtUWjNBWWlZREkvT1cxRVdIRkJ4Z08xdlM5QUtPajVQOStLbWdGOUNUK3JiMEtHZXlpOEZNak4rUWlyQUZtblJEdER0UklqVFRHTm53djQzS2w1ejdhY2VXejQxTm0wMGtESmFVUm80a2l5R3psYnBvTkNBZmR6ZlVaaHlDTGFtNU03NzlTT0pUb3dBQUFJQWxVUVZzOGRUT1RZdzdia0Z1OVRVOC9zTGxEME1LdUlZdnk4L1pmb29DSWplZE5HaUo5eVhmSHlUcWJuRWZiMXU4dGovbU5yMFZQZXUwd0t0Yk5xdUV3c3FxclhCOCt6cjJaTWE5Wmg4ZHhiUytDajlVSEtTSllzbUpkSjQ1SkhRYytPVDcwN21PTHBZd1JYYzYyU0lKNDFwY0VTc3Y2ckVzYXcwYTRqYTN4QT09IHJvb3RAb3dtZGVtbwo=
  id_rsa_key: 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
  id_dsa_key: LS0tLS1CRUdJTiBEU0EgUFJJVkFURSBLRVktLS0tLQpNSUlCdWdJQkFBS0JnUUM2KzNER3Fpd2hyS3NHbTBhT1Z5K3AvbjRqSVZqem9sTTFQN2YwOU85bGt6WVpWeUZxCkp1cS9sUktFd1d1MzFPU0tVaWtCTnlxOWlpOHJpdWs4QUE4VlZwZEhKY3prTEovVXovOGFFRkhFdkowR1RKTGsKZWcrLzFnM2svQUdWMTE0QzZ1MjRlbVUyQjdvdzFNQXh2TTgwZFVPY3plTThCSStKS3pMdEVxdG1qd0lWQUxqaQpiNkJLcG9zWWpYc2VlUVdtUUg1dHJHWnBBb0dBVzhjd1pTUnBDR1pPUlJzeUdoanNLM2t6RXNOYkNzcTFaRTJkCndHSW1BeVB6bHRSRmh4UWNZRHRiMHZRQ2pvK1QvZmlwb0JmUWsvcTI5Q2huc292QlRJemZrSXF3QlpwMFE3UTcKVVNJMDB4alo4TCtOeXBlYysybkhscytOVFp0TkpBeVdsRWFPSklzaHM1VzZhRFFnSDNjMzFHWWNnaTJwdVRPKwovVWppVTZNQ2dZQWxVUVZzOGRUT1RZdzdia0Z1OVRVOC9zTGxEME1LdUlZdnk4L1pmb29DSWplZE5HaUo5eVhmCkh5VHFibkVmYjF1OHRqL21OcjBWUGV1MHdLdGJOcXVFd3NxcXJYQjgrenIyWk1hOVpoOGR4YlMrQ2o5VUhLU0oKWXNtSmRKNDVKSFFjK09UNzA3bU9McFl3UlhjNjJTSUo0MXBjRVNzdjZyRXNhdzBhNGphM3hBSVVkbGRoZ2ROZApsSGJNM1orSkJ4dU9jWUNxVkg4PQotLS0tLUVORCBEU0EgUFJJVkFURSBLRVktLS0tLQo=
  dnsTimeout: 1

  ################################################################################################
  #  Values  common to ems01/ems02 deployments
  ###############################################################################################
  # List of service IPs and hostnames for CPXes in regions other than this one.
  # This is to enable Prometheus on EMS to scrape data from defined targets.
  # If config proxies are deployed then this will contain the service IP and hostname of all
  # config proxies in all other regions.
  # 
  # By default this is empty for a single region as the EMS can access the CPXs in its own region.
  #
  # Example for two region deployment:
  # Other region is region2 which has two CPXes with service IPs 192.168.230.143 and 192.168.230.144
  #
  #ems:
  #  otherRegionsPrometheusScrapeTargets:
  #  - ip: 192.168.230.143
  #    hostname: region1-stratum-cpx-0
  #  - ip: 192.168.230.144
  #    hostname: region1-stratum-cpx-1
  ems:
     otherRegionsPrometheusScrapeTargets:
     instance_id_start: 1000
  ems01:
    enabled: true

  ems02: 
    enabled: true
 
  # if ems01 is enabled in this region, creates a service allowing internal access on DNS name ems01, and external access via the loadbalancer I{.
  # same applies for ems02. 
  emsServices:
     loadBalancerIPs:
        ems01: [  192.168.120.180 ]    # multiple IPs may be specfied and a service will be created for each. The first service is named ems01; latter services ems01-[x]
        ems02: [  192.168.120.190 ]    # ditto
     ports:
       - name: cs
         port: 8080
         targetPort: 8080
       - name: https
         port: 8443
         targetPort: 8443
       - name: grafana
         port: 3000
         targetPort: 3000
       - name: ssh
         port: 22
         targetPort: 22
       - name: netdata
         port: 19999
         targetPort: 19999
       - name: aggregator
         port: 9650
         targetPort: 9650      
       - name: ordb
         port: 3306
         targetPort: 3306
       - name: dfmhb
         port: 10500
         targetPort: 10500


   # headless service for DNS entries
  headlessService:
    name: stratum-nodes
    selectorAppName: stratum

  ################################################################################################
  #  Ild service is created outside the replica setup, as it will serve multiple replicas
  ################################################################################################
  ild:
    pod_name: ild
  ildService:
    name: enea-ild
    loadBalancerIPs: [ 192.168.120.141, 192.168.120.162 ]
    ldap:
      port: 389
      targetPort: 3389
    sbi:
      port: 443
      targetPort: 3443
    extraPorts:
      - name: swc-tls
        port: 8443
        protocol: TCP
        targetPort: 3443
      - name: h2c-pcf
        port: 8001
        protocol: TCP
        targetPort: 3480
      - name: h2c-udm
        port: 8002
        protocol: TCP
        targetPort: 3480
      - name: h2c
        port: 8003
        protocol: TCP
        targetPort: 3480
  kubectlServiceAccountName: sa-kubectl

  ################################################################################################
  #  Values for daemonset running on each node
  #  Initially to set sysctls on the worker nodes required for Stratum to run
  ################################################################################################

  daemonset:
    ctr_name: stratum-daemonset
    pod_name: stratum-daemonset

    # Just need a minimal image to run the script
    # Use same RH image as init container images for regions
    image: registry.access.redhat.com/rhel7:latest
    
    # Probe configuration
    # Kubernetes defaults to 1 second which may not be long enough
    readinessProbeTimeout: 2
    livenessProbeTimeout: 2
    startupProbeTimeout: 2

    # How often the probes run (seconds)
    livenessProbePeriod: 10
    startupProbePeriod: 10

    # Number of failures before abort
    livenessFailureThreshold: 30
    startupFailureThreshold: 60    

    # Resources for the daemonset
    # Minimum as just runs a script in a container  
    cpu_request: 200m
    cpu_limit: 200m
    mem_request: 200M
    mem_limit: 200M

    # Script to run in the daemonset
    # Loaded into a configmap which then mounts that as a file in the container
    # Script runs in a loop and sets the sysctl value every n seconds
    # Other sysctls (or other functions) can be added to the script if required via this configmap

    script: |
      #!/bin/bash

      declare -i max_buffer_size=20971520
      declare -i sleep_time=60

      echo "Running Stratum daemonset script"

      while true
      do
        # Set the maximum read and write buffer sizes for Stratum
        sysctl -w net.core.rmem_max=${max_buffer_size}
        sysctl -w net.core.wmem_max=${max_buffer_size}
        sleep ${sleep_time}
      done

  # controls creation of ingress and service objects for stratum partitioning. 
  # sites:
  #    one of the sites must be the local one, its name matching the global.site value in this chart
  #    for the this site, create a service matching the ild service
  #    for a remote site, create a service with no selector, and an endpoint slice pointing at the remote site.
  #    all objects are named <site>-ingress-[local|remote]
  #    the names of the services are used as the backend names in haproxy;
  # partitions:
  #    this section lists the imsi ranges that form a data partition,
  #    and then the sites which should be queries in order for those (the order facilicates failover, if a particular site is down)
  #    this is used to generate an imsi/site mapping file used internally by haproxy.
  # example: 
#  partitioning:
#    partitions:
#      partition1:
#        imsiPatterns:
#        - ^[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]00[0-9][0-9][0-9]$
#        - ^[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]03[0-9][0-9][0-9]$
#        sitesByPriority:   <-- priorities will change per site install (e.g. when installing site3, site3 will come first here) 
#        - site1
#        - site3
#     partition2:           
#        imsiPatterns:
#        - ^[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]10[0-9][0-9][0-9]$
#        - ^[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]13[0-9][0-9][0-9]$  
#        sitesByPriority:     
#        - site2
#        - site4
#    sites:    
#    - name:          site1
#      ingress_ip:    192.168.130.8
#    - name:          site2
#      ingress_ip:    192.168.140.8
#    - name:          site3
#      ingress_ip:    192.168.150.8
#    - name:          site4
#  ingress_ip:    192.168.160.8
  partitioning:
    ingressClassName: haproxy  # QE: must be set if deploying multiple sites to 1 cluster.  ideally set to to <sitename>-ingress. must match haproxy-ingress.controller.ingressClass
    partitions:
    sites:

# Below shows example value overrides if running multiple sites in 1 cluster. Each site is installed to its own namespace with its own haproxy instance.
# However ingressclass resources are cluster level objects, not namespaced, and so settings must be overriden to create an ingress class for each namespace.
# these overrides should be done for every site's values. 
#haproxy-ingress:
#  controller:
#    ingressClass: site1-ingress
#    ingressClassResource:
#      controllerClass: site1-ingress
#      enabled: true
#    loadBalancerIP: 192.168.130.8   # must match IP given for this site in global.partitioning.sites
#    kind: Deployment
#    hostNetwork: false

# end example

haproxy-ingress:
  controller:
#    ingressClass:         # QE : must be set if deploying multiple sites to 1 cluster. ideally set to to <sitename>-ingress. must match global.partitioning.ingressClassName.
#    ingressClassResource:
#      controllerClass: site1-ingress   # QE : must be set if deploying multiple sites to 1 cluster. ideally set to to <sitename>-ingress. must match global.partitioning.ingressClassName.
#      enabled: true                    # QE : must be set if deploying multiple sites to 1 cluster.
    kind: DaemonSet        # QE : if deploying multiple sites to 1 cluster in a non perfomance / production env, change this to 'Deployment'
    nodeSelector:  {}      # should be set if deploying multiple sites to 1 cluster in a performance / production env
    hostNetwork: true      # QE : change to false if deploying multiple sites to 1 cluster
#   image:
#     registry:
#     repository: 
#     tag:
    resources:
      requests:
        cpu: 0.25
        memory: 500Mi
      limits:
        cpu: 0.25
        memory: 500Mi
    ingressClassResource:
      enabled: true
    defaultBackendService: haproxy-no-partition
    stats:
      enabled: true
    metrics:
      enabled: true
    serviceMonitor:
      enabled: true
      labels:
        release: prometheus
      metrics:
        relabelings:
        - replacement: cl1
          targetLabel: cluster
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: hostname
      ctrlMetrics:
        relabelings:
        - replacement: cl1
          targetLabel: cluster
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: hostname
    service:
      extraPorts:
      - name: http-8001
        port: 8001
        protocol: TCP
        targetPort: http
      externalTrafficPolicy: Cluster
      loadBalancerIP: 192.168.130.8
    logs:
      enabled: true
      resources:
        requests:
          cpu: 0.25
          memory: 500Mi
        limits:
          cpu: 0.25
          memory: 500Mi
#      image:
#       registry:
#       repository:
    haproxy:
      enabled: true
      securityContext:
        runAsUser: 0
      resources:
       limits:
         cpu: 1
         memory: 1Gi
       requests:
          cpu: 1
          memory: 1Gi
      image:
#       registry:
#       repository:  
        tag: 2.8.5
      lifecycle:
        preStop:
          exec:
            command: ["/bin/sh","-c","sleep 10; kill -SIGUSR1 $(pidof haproxy)"]
    config:
      load-balance: leastconn
      health-check-interval: "1s"
      slots-min-free: "0"
      max-connections: "100000"
      maxconn-server: "5000"
      config-global: |
        lua-prepend-path /etc/opwv/?.lua
        lua-load /etc/opwv/stratum.lua
        tune.h2.max-concurrent-streams 1000
        tune.h2.initial-window-size 1048576
        tune.bufsize 131072
        nbthread 1
      config-defaults: |
        option http-no-delay
        timeout connect 200
        timeout client 300s
        timeout server 5000
        option dontlog-normal
        no option prefer-last-server
        timeout client-fin 1s
        timeout server-fin 1s
        no option http-server-close
      config-frontend-early: |
        # Stratum: Begin
        # Define the ACLs first.
        # Used to determine if the IMSI is in the URL path or query parameters.
        acl has_imsi_in_url url_reg -i imsi-\d+

        # We use this ACL to extract the IMSI from the body. The same ACL is intentionally used twice.
        # With named ACLs, specifying the same ACL name multiple times will cause a logical OR of the conditions.
        # 1. /nudr-dr/v2/policy-data/subs-to-notify
        # 2. /nudr-dr/v2/subscription-data/subs-to-notify
        # 3. /nudr-dr/v2/application-data
        acl check_body_for_imsi path_reg -i subs-to-notify
        acl check_body_for_imsi path_reg -i \/nudr-dr\/v.*\/application-data

        # UDSF transactions should only be reouted to the local partition.
        acl is_udsf path -i -m beg /nudsf-dr

        # All roaming requests should be routed to the local partition too:
        acl is_roaming path -i -m reg /roaming-plan-profile

        # Used to detect if we already have the UUID header.
        acl no_stratum_partition_uuid hdr_cnt(stratum-partition-uuid) eq 0
        # Used to determine if we already have the header with the ingress start time.
        acl no_stratum_start hdr_cnt(stratum-ingress-start) eq 0

        # ACL to detect ILD health checks. We will use this to send to the local backend.
        acl is_georegion_health_check path -i -m beg /oam/v1/operational_status
        # ILD operational status checks should be routed to local ILDs.
        acl is_operational_status path -i -m beg /builtin/v1/operationalstatus

        # Get the IMSI from the body of a POST request.
        # We only buffer the request if we know what type of request it is.
        http-request wait-for-body time 5s at-least 16k if check_body_for_imsi
        http-request set-var(req.body) req.body if check_body_for_imsi

        # Store data that we need for the Lua scripts.
        http-request set-var(req.client_ip) fc_src        # The client IP address.
        http-request set-var(req.client_port) fc_src_port # The client port.
        http-request set-var(req.url) url                 # Store the full URL.
        http-request set-var(req.path) path               # The URL path.
        http-request set-var(req.method) method           # The HTTP method - to detect DELETE requests.
        http-request set-var(req.existing_uuid) req.hdr(stratum-partition-uuid)

        # Response variables: Begin
        # We use these for logging and diagnostics purposes.
        http-response set-var(res.client_ip) fc_src
        http-response set-var(res.client_port) fc_src_port
        http-response set-var(res.method) method
        http-response set-var(res.uuid) res.hdr(stratum-partition-uuid)
        http-response set-var(res.health_check) res.hdr(stratum-health-check)
        http-response set-var(res.start_time) res.hdr(stratum-ingress-start)
        # Response variables: End

        # If the stratum-specialattributes:2 header is set then we can trace the transaction in detail.
        http-request set-var(req.partition_trace_enabled) req.hdr(stratum-specialattributes)

        # Record where we came from if sending from one haproxy to another:
        http-request set-var(req.proxy_origin) req.hdr(stratum-partition-origin)

        # Assign a unique id to the transaction and add it to the request.
        http-request set-var(req.uuid) uuid() if no_stratum_partition_uuid
        # Only add the UUID header if it doesn't exist. It may already have been set by an upstream
        # HAProxy which we must preserve.
        http-request add-header stratum-partition-uuid %[var(req.uuid)] if no_stratum_partition_uuid

        # Add the epoch timestamp (ms) to the request headers. Only add this header if
        # it hasn't already been set by an upstream HAProxy.
        http-request add-header stratum-ingress-start %[date(0,us)] if no_stratum_start

        # Used to determine if requests should be routed only to the local partition.
        http-request set-var(req.local_partition_only) str("true") if is_udsf or is_roaming or is_georegion_health_check or is_operational_status
        http-request set-var(req.is_udsf) str("true") if is_udsf
        http-request set-var(req.is_health_check) str("true") if is_georegion_health_check

        # Request handler: Get the IMSI from the URL path, and store it in 'req.imsi'
        http-request lua.get_imsi_from_url if has_imsi_in_url

        # If we can't find the IMSI in the URL path, then we scan the entire request.
        # Note that we check to see if we have already obtained the IMSI from the URL path first.
        # If the request is a GET then we check the query parameters.
        # If the request is a DELETE we check to see if the IMSI is in a base64 encoded part of the path.
        # If the request if a POST then we check the request body.
        http-request lua.scan_request_for_imsi if check_body_for_imsi

        # Response handler
        http-response lua.handle_response

        # The Lua script will filter all requests.
        # 1. Requests with an IMSI will be routed to the mapped partition.
        # 2. If no mapping is found then the request is rejected.
        # 3. Any request that doesn't contain an IMSI is sent to the local partition.
        use_backend %[lua.select_partition]
        # Stratum: End

  
    extraVolumes:
      - name: opwv-files
        configMap:
          defaultMode: 420
          name: opwv-haproxy-files
        name: opwv-files
    extraVolumeMounts:
      - mountPath: /etc/opwv
        name: opwv-files
  
  
  defaultBackend:
    enabled: true
    replicaCount: 0
    name: no-partition
